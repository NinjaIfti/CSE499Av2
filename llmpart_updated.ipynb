{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Service for Lecture AI\n",
    "\n",
    "This notebook implements the LLM service endpoints required by the Flask backend:\n",
    "- `/process` - Merges OCR + transcript and generates structured notes\n",
    "- `/chat` - Answers questions about lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# INSTALL PACKAGES\n",
    "# ==============================\n",
    "\n",
    "# Install PyTorch with CUDA\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install other libraries\n",
    "!pip install -q fastapi uvicorn pyngrok nest_asyncio transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# IMPORTS\n",
    "# ==============================\n",
    "import nest_asyncio\n",
    "import threading\n",
    "import time\n",
    "import torch\n",
    "import json\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, List, Optional, Any\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# LOAD MODEL\n",
    "# ==============================\n",
    "print(\"Loading model...\")\n",
    "\n",
    "model_name = \"google/long-t5-tglobal-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# HELPER FUNCTIONS\n",
    "# ==============================\n",
    "\n",
    "def generate_summary(text: str, max_length: int = 200, min_length: int = 50) -> str:\n",
    "    \"\"\"Generate summary using the loaded model\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=2048\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def extract_key_points(text: str) -> List[str]:\n",
    "    \"\"\"Extract key points from text (simplified - can be enhanced)\"\"\"\n",
    "    sentences = text.split('.')\n",
    "    key_points = [s.strip() for s in sentences if len(s.strip()) > 20][:5]\n",
    "    return key_points\n",
    "\n",
    "def merge_ocr_transcript(ocr_data: Dict, transcript_data: Dict) -> str:\n",
    "    \"\"\"Merge OCR and transcript data into a single text\"\"\"\n",
    "    merged_text = []\n",
    "    \n",
    "    if ocr_data:\n",
    "        if isinstance(ocr_data, dict):\n",
    "            ocr_text = ocr_data.get('text', '') or json.dumps(ocr_data)\n",
    "        else:\n",
    "            ocr_text = str(ocr_data)\n",
    "        merged_text.append(f\"Board/Slide Content:\\n{ocr_text}\")\n",
    "    \n",
    "    if transcript_data:\n",
    "        if isinstance(transcript_data, dict):\n",
    "            transcript_text = transcript_data.get('text', '') or transcript_data.get('transcript', '') or json.dumps(transcript_data)\n",
    "        else:\n",
    "            transcript_text = str(transcript_data)\n",
    "        merged_text.append(f\"\\nTranscript:\\n{transcript_text}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(merged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# FASTAPI APP & MODELS\n",
    "# ==============================\n",
    "app = FastAPI()\n",
    "\n",
    "class ProcessRequest(BaseModel):\n",
    "    job_id: str\n",
    "    ocr_output: Dict[str, Any]\n",
    "    transcript: Dict[str, Any]\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    lecture_id: str\n",
    "    question: str\n",
    "    context: Dict[str, Any]\n",
    "    history: List[Dict[str, str]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# API ENDPOINTS\n",
    "# ==============================\n",
    "\n",
    "@app.get(\"/\")\n",
    "def home():\n",
    "    return {\"message\": \"Lecture AI LLM Service Running\"}\n",
    "\n",
    "@app.post(\"/process\")\n",
    "def process_lecture(request: ProcessRequest):\n",
    "    \"\"\"\n",
    "    Process OCR and transcript data to generate structured notes.\n",
    "    Expected by Flask backend: POST /process\n",
    "    \"\"\"\n",
    "    try:\n",
    "        merged_text = merge_ocr_transcript(request.ocr_output, request.transcript)\n",
    "        \n",
    "        summary = generate_summary(merged_text, max_length=300, min_length=100)\n",
    "        key_points = extract_key_points(merged_text)\n",
    "        \n",
    "        response = {\n",
    "            \"summary\": summary,\n",
    "            \"key_points\": key_points,\n",
    "            \"notes\": {\n",
    "                \"ocr_content\": request.ocr_output,\n",
    "                \"transcript_content\": request.transcript,\n",
    "                \"merged_text\": merged_text[:1000] + \"...\" if len(merged_text) > 1000 else merged_text\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Processing error: {str(e)}\")\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "def chat(request: ChatRequest):\n",
    "    \"\"\"\n",
    "    Answer questions about a lecture using context.\n",
    "    Expected by Flask backend: POST /chat\n",
    "    \"\"\"\n",
    "    try:\n",
    "        context = request.context\n",
    "        \n",
    "        summary = context.get('summary', '')\n",
    "        notes = context.get('notes', {})\n",
    "        transcript = context.get('transcript', {})\n",
    "        \n",
    "        context_text = f\"Summary: {summary}\\n\\n\"\n",
    "        \n",
    "        if isinstance(notes, dict):\n",
    "            notes_text = json.dumps(notes)[:500]\n",
    "            context_text += f\"Notes: {notes_text}\\n\\n\"\n",
    "        \n",
    "        if isinstance(transcript, dict):\n",
    "            transcript_text = transcript.get('text', '') or json.dumps(transcript)[:500]\n",
    "            context_text += f\"Transcript: {transcript_text}\\n\\n\"\n",
    "        \n",
    "        prompt = f\"{context_text}\\n\\nQuestion: {request.question}\\n\\nAnswer:\"\n",
    "        \n",
    "        answer = generate_summary(prompt, max_length=200, min_length=30)\n",
    "        \n",
    "        return {\"answer\": answer}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Chat error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# START SERVER\n",
    "# ==============================\n",
    "def run():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "thread = threading.Thread(target=run, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# ==============================\n",
    "# START NGROK\n",
    "# ==============================\n",
    "# Replace with your ngrok auth token\n",
    "ngrok.set_auth_token(\"YOUR_NGROK_AUTH_TOKEN_HERE\")\n",
    "public_url = ngrok.connect(8000)\n",
    "\n",
    "print(\"\\nðŸš€ LLM Service LIVE at:\")\n",
    "print(public_url)\n",
    "print(\"\\nðŸ“‹ Update your Flask .env file:\")\n",
    "print(f\"LLM_SERVICE_URL={public_url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
